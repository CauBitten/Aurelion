from src.layer1_distilbert import FastDetector
# Importamos a camada 2 apenas se necessÃ¡rio ou inicializamos lazy, 
# mas aqui vamos instanciar para demonstraÃ§Ã£o.
from src.layer2_llamaguard import RobustGuard

class Gaelion:
    def __init__(self, hf_token):
        print('ðŸ›¡ï¸ Inicializando Gaelion System...')
        # Inicializa a camada rÃ¡pida
        self.fast_layer = FastDetector()
        
        # Inicializa a camada robusta (Llama Guard)
        # Nota: Em produÃ§Ã£o, vocÃª poderia carregar isso em outro serviÃ§o/API para economizar RAM
        self.robust_layer = RobustGuard(hf_token=hf_token)
        
        # Limiares de decisÃ£o
        self.safe_threshold = 0.10  # Abaixo disso, confia que Ã© seguro
        self.danger_threshold = 0.90 # Acima disso, confia que Ã© ataque

    def check_prompt(self, prompt: str):
        print(f'\n--- Analisando: \'{prompt}\' ---')
        
        # 1. Camada RÃ¡pida (DistilBERT)
        risk_score = self.fast_layer.predict(prompt)
        print(f'âš¡ Camada 1 (DistilBERT) Risco: {risk_score:.4f}')

        # LÃ³gica de Funil (Pipeline HÃ­brido)
        
        # Caso A: Ataque Ã“bvio (Fast Reject)
        if risk_score > self.danger_threshold:
            return {
                'status': 'BLOCKED',
                'layer': 'Layer 1 (Fast)',
                'reason': 'High confidence malware signature',
                'risk_score': risk_score
            }

        # Caso B: Seguro Ã“bvio (Fast Pass)
        if risk_score < self.safe_threshold:
            return {
                'status': 'ALLOWED',
                'layer': 'Layer 1 (Fast)',
                'reason': 'Low risk detected',
                'risk_score': risk_score
            }

        # Caso C: Incerteza -> Chama o Especialista (Llama Guard)
        print('Incerteza na Camada 1. Acionando Llama Guard...')
        guard_verdict = self.robust_layer.evaluate(prompt)
        
        if 'unsafe' in guard_verdict:
            return {
                'status': 'BLOCKED',
                'layer': 'Layer 2 (Llama Guard)',
                'reason': 'Semantic violation detected',
                'details': guard_verdict
            }
        else:
            return {
                'status': 'ALLOWED',
                'layer': 'Layer 2 (Llama Guard)',
                'reason': 'Verified safe by expert model',
                'details': guard_verdict
            }